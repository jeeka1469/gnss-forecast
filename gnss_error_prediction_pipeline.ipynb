{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e45ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Embedding, Flatten\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"TensorFlow not available, using scikit-learn models\")\n",
    "\n",
    "try:\n",
    "    from preprocessing import GNSSErrorPreprocessor, create_train_test_split\n",
    "    print(\"Custom preprocessing module imported\")\n",
    "except ImportError:\n",
    "    print(\"Custom preprocessing module not found, will define functions inline\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5562a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('errors_day187_192.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.sort_values(['satellite_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"Satellites: {len(df['satellite_id'].unique())}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d43fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Information:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percentage\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "print(\"\\nSatellite Distribution:\")\n",
    "satellite_counts = df['satellite_id'].value_counts().sort_index()\n",
    "print(satellite_counts)\n",
    "\n",
    "print(\"\\nTemporal Distribution:\")\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "day_counts = df['day'].value_counts().sort_index()\n",
    "print(\"Records per day:\")\n",
    "for day, count in day_counts.items():\n",
    "    print(f\"  Day {day}: {count:,} records\")\n",
    "\n",
    "print(\"\\nTarget Variable Ranges:\")\n",
    "print(f\"Orbit Error (m): {df['orbit_error_m'].min():.3f} to {df['orbit_error_m'].max():.3f}\")\n",
    "print(f\"Clock Error (ns): {df['clock_error_ns'].min():.3f} to {df['clock_error_ns'].max():.3f}\")\n",
    "print(f\"Radial Error (m): {df['radial_error_m'].min():.3f} to {df['radial_error_m'].max():.3f}\")\n",
    "print(f\"Ephemeris Age (hrs): {df['ephemeris_age_hours'].min():.3f} to {df['ephemeris_age_hours'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "axes[0, 0].hist(df['orbit_error_m'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Distribution of Orbit Errors', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Orbit Error (m)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(df['clock_error_ns'], bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[0, 1].set_title('Distribution of Clock Errors', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Clock Error (ns)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].hist(df['radial_error_m'], bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_title('Distribution of Radial Errors', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Radial Error (m)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].hist(df['ephemeris_age_hours'], bins=50, alpha=0.7, color='gold', edgecolor='black')\n",
    "axes[1, 1].set_title('Distribution of Ephemeris Age', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Ephemeris Age (hours)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "correlation_matrix = df[['orbit_error_m', 'clock_error_ns', 'radial_error_m', 'ephemeris_age_hours']].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bbad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Cleaning:\")\n",
    "print(f\"Initial dataset shape: {df.shape}\")\n",
    "initial_records = len(df)\n",
    "\n",
    "print(f\"\\nMissing values check:\")\n",
    "nan_counts = df.isnull().sum()\n",
    "for col, count in nan_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {col}: {count} NaN values ({count/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nInfinite values check:\")\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    inf_count = np.isinf(df[col]).sum()\n",
    "    if inf_count > 0:\n",
    "        print(f\"  {col}: {inf_count} infinite values\")\n",
    "\n",
    "critical_columns = ['orbit_error_m', 'clock_error_ns', 'satellite_id', 'timestamp']\n",
    "df_clean = df.dropna(subset=critical_columns)\n",
    "\n",
    "print(f\"\\nAfter removing NaN values:\")\n",
    "print(f\"  Records removed: {initial_records - len(df_clean)}\")\n",
    "print(f\"  Remaining records: {len(df_clean)}\")\n",
    "print(f\"  Data retention: {len(df_clean)/initial_records*100:.2f}%\")\n",
    "\n",
    "# Detect outliers using IQR method\n",
    "def detect_outliers(series, k=1.5):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - k * IQR\n",
    "def detect_outliers(series, k=1.5):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - k * IQR\n",
    "    upper_bound = Q3 + k * IQR\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "print(f\"\\nOutlier Detection:\")\n",
    "outlier_columns = ['orbit_error_m', 'clock_error_ns', 'radial_error_m']\n",
    "outlier_masks = {}\n",
    "    outlier_percentage = outlier_count / len(df_clean) * 100\n",
    "for col in outlier_columns:\n",
    "    outliers = detect_outliers(df_clean[col])\n",
    "    outlier_count = outliers.sum()\n",
    "    outlier_percentage = outlier_count / len(df_clean) * 100\n",
    "    outlier_masks[col] = outliers\n",
    "    \n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Outliers: {outlier_count} ({outlier_percentage:.2f}%)\")\n",
    "\n",
    "    if outlier_count > 0:\n",
    "        print(f\"  Range: {df_clean[col].min():.3f} to {df_clean[col].max():.3f}\")\n",
    "        print(f\"  Outlier range: {df_clean.loc[outliers, col].min():.3f} to {df_clean.loc[outliers, col].max():.3f}\")\n",
    "\n",
    "combined_outliers = np.logical_or.reduce(list(outlier_masks.values()))\n",
    "print(f\"\\nTotal rows with outliers: {combined_outliers.sum()} ({combined_outliers.sum()/len(df_clean)*100:.2f}%)\")\n",
    "df_clean['has_outlier'] = combined_outliers\n",
    "df_clean['has_outlier'] = combined_outliers\n",
    "\n",
    "print(f\"\\nData Quality Check:\")\n",
    "print(f\"Final dataset shape: {df_clean.shape}\")\n",
    "print(f\"No missing values in critical columns: {df_clean[critical_columns].isnull().sum().sum() == 0}\")\n",
    "print(f\"Temporal continuity maintained: {df_clean['timestamp'].is_monotonic_increasing}\")\n",
    "print(f\"All satellites present: {len(df_clean['satellite_id'].unique())} satellites\")\n",
    "print(f\"All satellites present: {len(df_clean['satellite_id'].unique())} satellites\")\n",
    "df = df_clean.copy()\n",
    "print(f\"\\nReady for preprocessing with {len(df):,} clean records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Engineering:\")\n",
    "\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day_of_year'] = df['timestamp'].dt.dayofyear\n",
    "df['minute_of_day'] = df['timestamp'].dt.hour * 60 + df['timestamp'].dt.minute\n",
    "df['satellite_constellation'] = df['satellite_id'].apply(lambda x: int(x[1:]) // 8)\n",
    "\n",
    "window_size = 16\n",
    "print(\"Calculating rolling statistics...\")\n",
    "rolling_features = []\n",
    "\n",
    "for satellite in df['satellite_id'].unique():\n",
    "    sat_data = df[df['satellite_id'] == satellite].copy()\n",
    "    sat_data = sat_data.sort_values('timestamp')\n",
    "    \n",
    "    sat_data['orbit_error_ma_4h'] = sat_data['orbit_error_m'].rolling(window=window_size, min_periods=1).mean()\n",
    "    sat_data['clock_error_ma_4h'] = sat_data['clock_error_ns'].rolling(window=window_size, min_periods=1).mean()\n",
    "    sat_data['orbit_error_std_4h'] = sat_data['orbit_error_m'].rolling(window=window_size, min_periods=1).std()\n",
    "    sat_data['clock_error_std_4h'] = sat_data['clock_error_ns'].rolling(window=window_size, min_periods=1).std()\n",
    "    sat_data['orbit_error_diff'] = sat_data['orbit_error_m'].diff()\n",
    "    sat_data['clock_error_diff'] = sat_data['clock_error_ns'].diff()\n",
    "    \n",
    "    rolling_features.append(sat_data)\n",
    "\n",
    "df_featured = pd.concat(rolling_features, ignore_index=True)\n",
    "df_featured = df_featured.sort_values(['satellite_id', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "df_featured['orbit_error_diff'] = df_featured['orbit_error_diff'].fillna(0)\n",
    "df_featured['clock_error_diff'] = df_featured['clock_error_diff'].fillna(0)\n",
    "df_featured['orbit_error_std_4h'] = df_featured['orbit_error_std_4h'].fillna(0)\n",
    "df_featured['clock_error_std_4h'] = df_featured['clock_error_std_4h'].fillna(0)\n",
    "\n",
    "print(f\"Feature engineering completed\")\n",
    "print(f\"Original features: {df.shape[1]}\")\n",
    "print(f\"Enhanced features: {df_featured.shape[1]}\")\n",
    "print(f\"New features added: {df_featured.shape[1] - df.shape[1]}\")\n",
    "\n",
    "new_features = [col for col in df_featured.columns if col not in df.columns]\n",
    "print(f\"\\nNew features: {new_features}\")\n",
    "\n",
    "df = df_featured.copy()\n",
    "\n",
    "# Show feature summary\n",
    "print(f\"\\n📊 Feature Summary:\")\n",
    "print(\"=\"*50)\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"Total features: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dba590",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train/Validation Split:\")\n",
    "\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "max_day = df['day'].max()\n",
    "train_df = df[df['day'] < max_day].copy()\n",
    "val_df = df[df['day'] == max_day].copy()\n",
    "\n",
    "print(f\"Training set:\")\n",
    "print(f\"  Period: {train_df['timestamp'].min()} to {train_df['timestamp'].max()}\")\n",
    "print(f\"  Records: {len(train_df):,}\")\n",
    "print(f\"  Days: {sorted(train_df['day'].unique())}\")\n",
    "print(f\"  Satellites: {len(train_df['satellite_id'].unique())}\")\n",
    "\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Period: {val_df['timestamp'].min()} to {val_df['timestamp'].max()}\")\n",
    "print(f\"  Records: {len(val_df):,}\")\n",
    "print(f\"  Days: {sorted(val_df['day'].unique())}\")\n",
    "print(f\"  Satellites: {len(val_df['satellite_id'].unique())}\")\n",
    "\n",
    "print(f\"\\nData Integrity Check:\")\n",
    "print(f\"  No temporal overlap: {train_df['timestamp'].max() < val_df['timestamp'].min()}\")\n",
    "print(f\"  All satellites in both sets: {set(train_df['satellite_id'].unique()) == set(val_df['satellite_id'].unique())}\")\n",
    "print(f\"  Total records: {len(train_df) + len(val_df)} (original: {len(df)})\")\n",
    "\n",
    "print(f\"\\nDay-based split complete: {len(train_df):,} training and {len(val_df):,} validation records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd8674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing Configuration:\")\n",
    "\n",
    "preprocessing_configs = [\n",
    "    {\n",
    "        'name': 'Standard Config',\n",
    "        'normalization_method': 'standard',\n",
    "        'encoding_method': 'embedding',\n",
    "        'lookback_window': 12,\n",
    "        'prediction_horizon': 1\n",
    "    },\n",
    "    {\n",
    "        'name': 'MinMax Config',\n",
    "        'normalization_method': 'minmax',\n",
    "        'encoding_method': 'embedding',\n",
    "        'lookback_window': 8,\n",
    "        'prediction_horizon': 1\n",
    "    },\n",
    "    {\n",
    "        'name': 'OneHot Config',\n",
    "        'normalization_method': 'standard',\n",
    "        'encoding_method': 'onehot',\n",
    "        'lookback_window': 6,\n",
    "        'prediction_horizon': 1\n",
    "    }\n",
    "]\n",
    "\n",
    "processed_datasets = {}\n",
    "\n",
    "for config in preprocessing_configs:\n",
    "    print(f\"\\nTesting {config['name']}:\")\n",
    "    print(f\"  Normalization: {config['normalization_method']}\")\n",
    "    print(f\"  Encoding: {config['encoding_method']}\")\n",
    "    print(f\"  Lookback window: {config['lookback_window']} steps ({config['lookback_window'] * 0.25:.1f} hours)\")\n",
    "    print(f\"  Prediction horizon: {config['prediction_horizon']} step(s)\")\n",
    "    \n",
    "    preprocessor = GNSSErrorPreprocessor(\n",
    "        normalization_method=config['normalization_method'],\n",
    "        encoding_method=config['encoding_method'],\n",
    "        lookback_window=config['lookback_window'],\n",
    "        prediction_horizon=config['prediction_horizon']\n",
    "    )\n",
    "    \n",
    "    core_columns = ['satellite_id', 'timestamp', 'orbit_error_m', 'clock_error_ns', 'ephemeris_age_hours']\n",
    "    train_core = train_df[core_columns].copy()\n",
    "    val_core = val_df[core_columns].copy()\n",
    "    \n",
    "    X_train, y_train = preprocessor.fit_transform(train_core)\n",
    "    X_val, y_val = preprocessor.transform(val_core)\n",
    "    \n",
    "    processed_datasets[config['name']] = {\n",
    "        'preprocessor': preprocessor,\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_val': X_val,\n",
    "        'y_val': y_val,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    print(f\"  Training sequences: {X_train.shape}\")\n",
    "    print(f\"  Validation sequences: {X_val.shape}\")\n",
    "\n",
    "selected_config = 'Standard Config'\n",
    "selected_data = processed_datasets[selected_config]\n",
    "\n",
    "print(f\"\\nSelected Configuration: {selected_config}\")\n",
    "\n",
    "X_train = selected_data['X_train']\n",
    "y_train = selected_data['y_train']\n",
    "X_val = selected_data['X_val']\n",
    "y_val = selected_data['y_val']\n",
    "preprocessor = selected_data['preprocessor']\n",
    "\n",
    "print(f\"Training data shape:\")\n",
    "print(f\"  X_train: {X_train.shape} (samples, time_steps, features)\")\n",
    "print(f\"  y_train: {y_train.shape} (samples, prediction_steps, targets)\")\n",
    "\n",
    "print(f\"\\nValidation data shape:\")\n",
    "print(f\"  X_val: {X_val.shape}\")\n",
    "print(f\"  y_val: {y_val.shape}\")\n",
    "\n",
    "info = preprocessor.get_preprocessing_info()\n",
    "print(f\"\\nPreprocessor Details:\")\n",
    "for key, value in info.items():\n",
    "    if key not in ['satellite_mapping', 'feature_names']:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ffffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking for NaN values in training data...\")\n",
    "print(f\"X_train_ml has NaN: {np.isnan(X_train_ml).any()}\")\n",
    "print(f\"y_train_ml has NaN: {np.isnan(y_train_ml).any()}\")\n",
    "print(f\"X_val_ml has NaN: {np.isnan(X_val_ml).any()}\")\n",
    "print(f\"y_val_ml has NaN: {np.isnan(y_val_ml).any()}\")\n",
    "\n",
    "nan_counts_X_train = np.isnan(X_train_ml).sum()\n",
    "nan_counts_y_train = np.isnan(y_train_ml).sum()\n",
    "print(f\"\\nNaN counts in X_train: {nan_counts_X_train}\")\n",
    "print(f\"NaN counts in y_train: {nan_counts_y_train}\")\n",
    "\n",
    "print(\"\\nCleaning data by removing NaN values...\")\n",
    "valid_indices = ~(np.isnan(X_train_ml).any(axis=1) | np.isnan(y_train_ml).any(axis=1))\n",
    "X_train_clean = X_train_ml[valid_indices]\n",
    "y_train_clean = y_train_ml[valid_indices]\n",
    "\n",
    "valid_indices_val = ~(np.isnan(X_val_ml).any(axis=1) | np.isnan(y_val_ml).any(axis=1))\n",
    "X_val_clean = X_val_ml[valid_indices_val]\n",
    "y_val_clean = y_val_ml[valid_indices_val]\n",
    "\n",
    "print(f\"Original training samples: {X_train_ml.shape[0]}\")\n",
    "print(f\"Clean training samples: {X_train_clean.shape[0]}\")\n",
    "print(f\"Original validation samples: {X_val_ml.shape[0]}\")\n",
    "print(f\"Clean validation samples: {X_val_clean.shape[0]}\")\n",
    "\n",
    "X_train_ml = X_train_clean\n",
    "y_train_ml = y_train_clean\n",
    "X_val_ml = X_val_clean\n",
    "y_val_ml = y_val_clean\n",
    "\n",
    "print(f\"\\nFinal clean data shapes:\")\n",
    "print(f\"X_train: {X_train_ml.shape}, y_train: {y_train_ml.shape}\")\n",
    "print(f\"X_val: {X_val_ml.shape}, y_val: {y_val_ml.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Summary\n",
    "print(\"Preprocessing Complete\")\n",
    "print(f\"Training sequences: {X_train.shape}\")\n",
    "print(f\"Validation sequences: {X_val.shape}\")\n",
    "print(f\"Features per timestep: {X_train.shape[2]}\")\n",
    "print(f\"Sequence length: {X_train.shape[1]} timesteps\")\n",
    "print(f\"Target variables: {y_train.shape[1] if len(y_train.shape) > 1 else 1}\")\n",
    "\n",
    "# Data ready for model training\n",
    "print(\"\\\\nData is ready for model training...\")\n",
    "print(\"Next steps: Model definition, training, and evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e315e41d",
   "metadata": {},
   "source": [
    "## Model Training Section (Extensible)\n",
    "\n",
    "Add model training code below. The preprocessed data is ready:\n",
    "- `X_train`, `y_train`: Training sequences\n",
    "- `X_val`, `y_val`: Validation sequences  \n",
    "- `preprocessor`: Fitted preprocessor for inverse transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add model training code here\n",
    "# Available data:\n",
    "# - X_train: shape {X_train.shape if 'X_train' in locals() else 'Not defined'}\n",
    "# - y_train: shape {y_train.shape if 'y_train' in locals() else 'Not defined'}\n",
    "# - X_val: shape {X_val.shape if 'X_val' in locals() else 'Not defined'}  \n",
    "# - y_val: shape {y_val.shape if 'y_val' in locals() else 'Not defined'}\n",
    "\n",
    "# Example model training structure:\n",
    "# 1. Define model architecture\n",
    "# 2. Compile model  \n",
    "# 3. Train model\n",
    "# 4. Evaluate performance\n",
    "# 5. Save results\n",
    "\n",
    "pass  # Remove this when adding actual code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
